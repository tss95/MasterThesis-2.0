{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'C:\\\\Documents\\\\Thesis_ssd\\\\MasterThesis-2.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ca5d92a2137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mclasses_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:\\Documents\\Thesis_ssd\\MasterThesis-2.0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mClasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataProcessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadData\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLoadData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mClasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataProcessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaselineHelperFunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaselineHelperFunctions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Documents\\\\Thesis_ssd\\\\MasterThesis-2.0'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import Stream, Trace, UTCDateTime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Activation, Conv1D, Dense, Dropout, Flatten, MaxPooling3D, BatchNormalization, InputLayer, LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "classes_dir = 'C:\\Documents\\Thesis_ssd\\MasterThesis-2.0'\n",
    "os.chdir(classes_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.BaselineHelperFunctions import BaselineHelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.Modeling.Models import Models\n",
    "from Classes.Modeling.RandomGridSearch import RandomGridSearch\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "from Classes import Tf_shutup\n",
    "Tf_shutup.Tf_shutup()\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = BaselineHelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "base_dir = 'C:\\Documents\\Thesis_ssd\\MasterThesis'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "loadData = LoadData(num_classes = num_classes, isBalanced = True)\n",
    "shuffle = True\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.getDatasets(shuffle = shuffle)\n",
    "data_gen = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'model_nr': 5, 'index': 10}\n",
    "{'batch_size': 32, 'epochs': 35, 'learning_rate': 1e-05, 'optimizer': 'rmsprop'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.4, 'filters': 21, 'kernel_size': 7, 'l1_r': 0.001, 'l2_r': 0.2, \n",
    "'output_layer_activation': 'sigmoid', 'padding': 'same', 'start_neurons': 16}\n",
    "\n",
    "{'model_nr': 4, 'index': 10}\n",
    "{'batch_size': 8, 'epochs': 30, 'learning_rate': 0.0001, 'optimizer': 'adam'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.3, 'filters': 17, 'kernel_size': 3, 'l1_r': 0.01, 'l2_r': 0.1, \n",
    "'output_layer_activation': 'softmax', 'padding': 'same', 'start_neurons': 128}\n",
    "\n",
    "Crashing model:\n",
    "Test_mode: False, use_scaler: True, use_minmax: False, use_noise_augmentor: True, detrend: False\n",
    "{'model_nr': 4, 'index': 21}\n",
    "{'batch_size': 16, 'epochs': 35, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.01, 'filters': 15, 'kernel_size': 13, 'l1_r': 0.0001, \n",
    "'l2_r': 0.2, 'output_layer_activation': 'sigmoid', 'padding': 'same', 'start_neurons': 64}\n",
    "\n",
    "\n",
    "2 classes:\n",
    "{'model_nr': 7, 'index': 38}\n",
    "Test_mode: False, use_scaler: True, use_minmax: False, use_noise_augmentor: True detrend: False. \n",
    "{'batch_size': 64, 'epochs': 40, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\n",
    "{'activation': 'tanh', 'dropout_rate': 0, 'filters': 13, 'kernel_size': 5, 'l1_r': 0.0001,\n",
    "'l2_r': 0.01, 'output_layer_activation': 'softmax', 'padding': 'same', 'start_neurons': 32}\n",
    "{'test_loss': 1.4299607276916504, 'test_accuracy': 0.7728365659713745, \n",
    "'test_precision': 0.7728365659713745, 'test_recall': 0.7728365659713745}\n",
    "{'train_loss': 0.9629267454147339, 'train_accuracy': 0.8897058963775635, \n",
    "'train_precision': 0.8897058963775635, 'train_recall': 0.8897058963775635}\n",
    "\n",
    "\n",
    "2 classes:\n",
    "{'model_nr': 7, 'index': 31}\n",
    "{'batch_size': 128, 'epochs': 40, 'learning_rate': 0.1, 'optimizer': 'sgd'}\n",
    "{'activation': 'tanh', 'dropout_rate': 0.1, 'filters': 17, 'kernel_size': 7, \n",
    "'l1_r': 0.001, 'l2_r': 0.0001, 'output_layer_activation': 'sigmoid', 'padding': 'same', 'start_neurons': 8}\n",
    "{'test_loss': 1.1340930461883545, 'test_accuracy': 0.7511160969734192, \n",
    "'test_precision': 0.7511160969734192, 'test_recall': 0.7511160969734192}\n",
    "{'train_loss': 1.0580902099609375, 'train_accuracy': 0.7890625, \n",
    "'train_precision': 0.7890625, 'train_recall': 0.7890625}\n",
    "\n",
    "\"\"\"\n",
    "############ Model picker #############\n",
    "model_nr = 7\n",
    "\n",
    "########### Hyperparameters ###########\n",
    "batch_size = 64\n",
    "epochs = 80\n",
    "learning_rate = 0.001\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=1.0, clipvalue=0.5)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "activation = 'tanh'\n",
    "output_layer_activation = 'softmax'\n",
    "dropout_rate = 0\n",
    "filters = 13\n",
    "kernel_size = 5\n",
    "l1_r = 0.0001\n",
    "l2_r = 0.01\n",
    "padding = 'same'\n",
    "start_neurons = 32\n",
    "\n",
    "########### Preprocessing ###########\n",
    "test = False\n",
    "use_noise_augmentor = True\n",
    "detrend = False\n",
    "use_scaler = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.2\n",
    "\n",
    "use_tensorboard = True\n",
    "use_livelossplot = False\n",
    "use_custom = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "        \n",
    "if use_tensorboard:\n",
    "    import datetime\n",
    "    clear_tensorboard_dir()\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir tensorboard_dir/fit\n",
    "    log_dir = f\"{base_dir}/tensorboard_dir/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks = [tensorboard_callback]\n",
    "\n",
    "if use_custom:\n",
    "    custom_callback = CustomCallback(data_gen)\n",
    "    callbacks = custom_callback\n",
    "elif use_livelossplot:\n",
    "    callbacks = PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ds, channels, timesteps = data_gen.get_trace_shape_no_cast(train_ds)\n",
    "input_shape = (batch_size, channels, timesteps)\n",
    "\n",
    "build_model_args = {'model_nr' : model_nr,\n",
    "                    'input_shape' : input_shape,\n",
    "                    'num_classes' : num_classes,\n",
    "                    'dropout_rate' : dropout_rate,\n",
    "                    'activation' : activation,\n",
    "                    'output_layer_activation' : output_layer_activation,\n",
    "                    'l2_r' : l2_r,\n",
    "                    'l1_r' : l1_r,\n",
    "                    'full_regularizer' : True,\n",
    "                    'start_neurons' : start_neurons,\n",
    "                    'filters' : filters,\n",
    "                    'kernel_size' : kernel_size,\n",
    "                    'padding' : 'same'}\n",
    "model = Models(**build_model_args).model\n",
    "\n",
    "model_args = {'loss' : \"binary_crossentropy\",\n",
    "              'optimizer' : opt,\n",
    "              'metrics' : [\"accuracy\",\"MSE\",\n",
    "                           tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None),\n",
    "                           tf.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)]}\n",
    "model.compile(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if use_scaler:\n",
    "    scaler = StandardScalerFitter(train_ds).fit_scaler(test = test, detrend = detrend)\n",
    "aug = None\n",
    "if use_noise_augmentor:\n",
    "    aug = NoiseAugmentor(train_ds, use_scaler, scaler)\n",
    "    \n",
    "    \n",
    "\n",
    "gen_args = {\n",
    "    'batch_size' : batch_size,\n",
    "    'test' : test,\n",
    "    'detrend' : detrend,\n",
    "    'use_scaler' : use_scaler,\n",
    "    'scaler' : scaler,\n",
    "    'use_noise_augmentor' : use_noise_augmentor,\n",
    "    'augmentor' : aug,\n",
    "    'num_classes' : num_classes,\n",
    "    'use_highpass' : use_highpass,\n",
    "    'highpass_freq' : highpass_freq\n",
    "}\n",
    "\n",
    "\n",
    "train_gen = data_gen.data_generator(train_ds, **gen_args)\n",
    "val_gen = data_gen.data_generator(val_ds, **gen_args)\n",
    "test_gen = data_gen.data_generator(test_ds, **gen_args)\n",
    "\n",
    "\n",
    "\n",
    "args = {'steps_per_epoch' : helper.get_steps_per_epoch(train_ds, batch_size, test),\n",
    "        'epochs' : epochs,\n",
    "        'validation_data' : val_gen,\n",
    "        'validation_steps' : helper.get_steps_per_epoch(val_ds, batch_size, test),\n",
    "        'verbose' : 1,\n",
    "        'use_multiprocessing' : False, \n",
    "        'workers' : 1,\n",
    "        'callbacks' : [callbacks]\n",
    "}\n",
    "\n",
    "model_fit = model.fit(train_gen, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logs = custom_callback.full_training_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.get_n_points_with_highest_training_loss(train_ds, 100, full_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_points_with_highest_training_loss(full_logs, train_ds, n):\n",
    "    train_ds_dict = {}\n",
    "    for path, label in train_ds:\n",
    "        train_ds_dict[path] = {'label' : label,\n",
    "                               'loss': 0,\n",
    "                               'average_loss' : 0,\n",
    "                               'occurances' : 0}\n",
    "    counter = 0\n",
    "    for batch in full_logs:\n",
    "        loss = batch['loss']\n",
    "        for path_class in batch['batch_samples']:\n",
    "            train_ds_dict[path_class[0]]['loss'] += loss\n",
    "            train_ds_dict[path_class[0]]['occurances'] += 1\n",
    "    \n",
    "    train_ds_list = []\n",
    "    for sample in np.array(train_ds[:,0]):\n",
    "        if train_ds_dict[sample]['occurances'] == 0:\n",
    "            continue\n",
    "        train_ds_dict[sample]['average_loss'] = train_ds_dict[sample]['loss'] / train_ds_dict[sample]['occurances']\n",
    "        train_ds_list.append((sample, train_ds_dict[sample]['label'],train_ds_dict[sample]['average_loss']))\n",
    "    \n",
    "    sorted_train_ds_list = sorted(train_ds_list, key=lambda x: x[2], reverse = True)\n",
    "        \n",
    "    \n",
    "    return sorted_train_ds_list[0:n]\n",
    "        \n",
    "#get_n_points_with_highest_loss(full_logs, train_ds, 100)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=test_gen, steps=helper.get_steps_per_epoch(test_ds, batch_size, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true_categorical.argmax(axis=1), predictions[0:1234].argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_confusion_matrix(model, test_gen, test_ds, batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_generator(val_gen, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_confusion_matrix(test_ds, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
