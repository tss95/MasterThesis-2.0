{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'C:\\Documents\\Thesis_ssd\\Master Thesis\\data_tord_may2020'\n",
    "output_folder = 'C:\\Documents\\Thesis_ssd\\Master Thesis\\MasterThesis-2.0\\csv_folder'\n",
    "explo_path = f'{root}/explosions/'\n",
    "earth_path = f'{root}/earthquakes/'\n",
    "noise_path = f'{root}/noise/'\n",
    "induced_path = f'{root}/induced/'\n",
    "data_csv = 'event_paths_no_nan_no_induced.csv'\n",
    "balanced_csv = 'balanced_csv_3_class.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bypassing issues\n",
    " - Due to either dataset issues or ignorance, I was forced to label the dataset by their file names, rather than event info.\n",
    " - The dataset contains a lot of NaN values, and therefore need to have a method for omitting those datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event_csv(root, csv_name, output_folder):\n",
    "    with open(output_folder + '/' + csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ' ', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for event_type in os.listdir(root):\n",
    "            if event_type == 'induced':\n",
    "                continue\n",
    "            event_list = os.listdir(root+'/'+event_type)\n",
    "            for event in event_list:\n",
    "                path = f'{root}/{event_type}/{event}'\n",
    "                trace_array, label, info = path_to_trace(path)\n",
    "                if np.any(pd.isnull(trace_array)):\n",
    "                    continue\n",
    "                if label == \"induced or triggered event\":\n",
    "                    continue\n",
    "                else:\n",
    "                    writer.writerow(f'{root}/{event_type}/{event},{label}')\n",
    "            print(f'Completed {event_type}')\n",
    "    print(\"Completed \" + csv_name)\n",
    "    \n",
    "def csv_to_numpy(data_csv):\n",
    "    nr_rows = 0\n",
    "    with open(data_csv, 'r') as file:\n",
    "        csv_reader = csv.reader(file, delimiter = ',')\n",
    "        nr_rows = len(list(csv_reader))\n",
    "    dataset = np.empty((nr_rows, 2), dtype=object)\n",
    "    with open(data_csv, 'r') as file:\n",
    "        csv_reader = csv.reader(file, delimiter = ',')\n",
    "        idx = 0\n",
    "        for row in csv_reader:\n",
    "            dataset[idx][0] = row[0]\n",
    "            dataset[idx][1] = row[1]\n",
    "            idx += 1\n",
    "        np.random.shuffle(dataset)\n",
    "        return dataset\n",
    "    \n",
    "def path_to_trace(path):\n",
    "    trace_array = np.empty((3,6001))\n",
    "    with h5py.File(path, 'r') as dp:\n",
    "        trace_array[:3] = dp.get('traces')\n",
    "        info = np.array(dp.get('event_info'))\n",
    "        info = json.loads(str(info))\n",
    "    # No event type for noise, so handling that below\n",
    "    if path.split('/')[1] == 'noise':\n",
    "        label = 'noise'\n",
    "    else:\n",
    "        label = info['event_type']\n",
    "    # Since we consider induced earthquakes as earthquakes we need to handle that as well:\n",
    "    if label == \"induced or triggered event\":\n",
    "        label = \"earthquake\"\n",
    "    return trace_array, label, info\n",
    "\n",
    "def assert_true_labels(ds):\n",
    "    for path, label in ds:\n",
    "        _, label_from_trace, _ = path_to_trace(path)\n",
    "        if label_from_trace != 'noise':\n",
    "            label_from_trace = label_from_trace + 's'\n",
    "            label = label + 's'\n",
    "        assert path.split('/')[1] == label == label_from_trace, f'Mismatch between {path.split(\"/\")[1]} and {label} and {label_from_trace}, at path: {path}'\n",
    "\n",
    "\n",
    "def get_class_distribution_from_csv(data_csv):\n",
    "    with open(data_csv) as file:\n",
    "        nr_earthquakes = 0\n",
    "        nr_explosions = 0\n",
    "        nr_noise = 0\n",
    "        nr_total = 0\n",
    "        for row in file:\n",
    "            event_type = row.split(',')[1].rstrip()\n",
    "            if event_type == \"earthquake\":\n",
    "                nr_earthquakes += 1\n",
    "            elif event_type == \"explosion\":\n",
    "                nr_explosions += 1\n",
    "            elif event_type == \"noise\":            \n",
    "                nr_noise += 1\n",
    "            nr_total += 1\n",
    "        \n",
    "        return nr_earthquakes, nr_explosions, nr_noise, nr_total\n",
    "    \n",
    "def even_classes_csv(data_csv, balanced_csv_name):\n",
    "    with open(balanced_csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ' ', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        nr_earthquakes, nr_explosions, nr_noise, nr_total = get_class_distribution_from_csv(data_csv)\n",
    "        min_class_nr = min([nr_earthquakes, nr_explosions, nr_noise, nr_total])\n",
    "        print(min_class_nr)\n",
    "        csv_numpy = csv_to_numpy(data_csv)\n",
    "        class_count = []\n",
    "        pure_index_list = []\n",
    "        for event_type in os.listdir(root):\n",
    "            pure_index_single_event = []\n",
    "            if event_type == \"induced\":\n",
    "                continue\n",
    "                print(event_type)\n",
    "            else:\n",
    "                \n",
    "                print(len(csv_numpy))\n",
    "                print(f'{csv_numpy[100][1].rstrip()} vs {event_type}')\n",
    "                for line in range(len(csv_numpy)):\n",
    "                    \n",
    "                    if csv_numpy[line][1].rstrip() == event_type:\n",
    "                        pure_index_single_event.append(np.where(csv_numpy[line]))\n",
    "                pure_index_list.append(pure_index_single_event)\n",
    "        for pure_type_index_list in pure_index_list:\n",
    "            print(len(pure_type_index_list))\n",
    "                #random_samples = np.random.sample(pure_list, min_class_nr)\n",
    "                #class_count.append(len(random_samples))\n",
    "                #for line in range(len(random_samples)):\n",
    "                  #  writer.writerow([f'{random_samples[line][0]},{random_samples[line][1]}'])\n",
    "                \n",
    "        #print(class_count)   \n",
    "\n",
    "def even_classes_csv2(data_csv, balanced_csv_name, output_folder):\n",
    "    nr_earthquakes, nr_explosions, nr_noise, nr_total = get_class_distribution_from_csv(data_csv)\n",
    "    min_class_nr = min([nr_earthquakes, nr_explosions, nr_noise, nr_total])\n",
    "    print(min_class_nr)\n",
    "    csv_numpy = csv_to_numpy(data_csv)\n",
    "    class_count = [0,0,0,0]\n",
    "    earthquake_list = []\n",
    "    explosion_list = []\n",
    "    noise_list = []\n",
    "    pure_classes = []\n",
    "    final_class_count = []\n",
    "    for path, label in csv_numpy:\n",
    "        if label == \"explosion\":\n",
    "            explosion_list.append([path,label])\n",
    "            class_count[0] += 1\n",
    "            class_count[3] += 1\n",
    "        if label == \"earthquake\":\n",
    "            earthquake_list.append([path,label])\n",
    "            class_count[1] += 1\n",
    "            class_count[3] += 1\n",
    "        if label == \"noise\":\n",
    "            noise_list.append([path,label])\n",
    "            class_count[2] += 1\n",
    "            class_count[3] += 1\n",
    "    pure_classes.append(earthquake_list)\n",
    "    pure_classes.append(explosion_list)\n",
    "    pure_classes.append(noise_list)\n",
    "    print(len(pure_classes[0]))\n",
    "    with open(output_folder + '/' + balanced_csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ' ', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        for single_class in pure_classes:\n",
    "            print(f'Sample_size = {min_class_nr} single_class length: {len(single_class)}' )\n",
    "            random_samples = random.sample(single_class, min_class_nr)\n",
    "            final_class_count.append(len(random_samples))\n",
    "            for path, label in random_samples:\n",
    "                writer.writerow([f'{path},{label}'])\n",
    "        print(final_class_count)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "def generate_subset_csv(input_ds, output_csv_name, output_folder):\n",
    "    nr_rows = len(input_ds)\n",
    "    with open(output_folder + '/' + output_csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ',', quotechar = ' ', quoting = csv.QUOTE_MINIMAL)\n",
    "        for row in input_ds:\n",
    "            writer.writerow(row)\n",
    "    print(f'Completed writing {nr_rows} rows to {output_folder}/{output_csv_name}.')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed earthquakes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a645c3761744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_event_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-9266298190fd>\u001b[0m in \u001b[0;36mgenerate_event_csv\u001b[1;34m(root, csv_name, output_folder)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mevent_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'induced'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mevent_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mevent_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'{root}/{event_type}/{event}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_event_csv(root, data_csv, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquakes: 0, Explosions: 0, Noise: 0, Total: 206250\n"
     ]
    }
   ],
   "source": [
    "nr_eq, nr_ex, nr_noise, nr_total = get_class_distribution_from_csv(data_csv)\n",
    "print(f'Earthquakes: {nr_eq}, Explosions: {nr_ex}, Noise: {nr_noise}, Total: {nr_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "Sample_size = 0 single_class length: 0\n",
      "Sample_size = 0 single_class length: 0\n",
      "Sample_size = 0 single_class length: 0\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Generate balanced data csv using data_csv:\n",
    "even_classes_csv2(data_csv, balanced_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = csv_to_numpy(balanced_csv)\n",
    "\n",
    "train_ds, test_val_ds = train_test_split(ds, test_size = 0.2)\n",
    "val_ds, test_ds = train_test_split(test_val_ds, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16444 2878 1234\n",
      "20556\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "print(len(train_ds) + len(val_ds) + len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_true_labels(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating balanced splitted csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing 1234 rows to csv_folder_3_class/balanced/test_set.csv.\n",
      "Completed writing 2878 rows to csv_folder_3_class/balanced/validation_set.csv.\n",
      "Completed writing 16444 rows to csv_folder_3_class/balanced/train_set.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_sub = 'balanced'\n",
    "output_folder = f\"csv_folder_3_class/{root_sub}\"\n",
    "\n",
    "# Test set:\n",
    "output_csv_name = \"test_set.csv\"\n",
    "generate_subset_csv(test_ds, output_csv_name, output_folder)\n",
    "\n",
    "# Validation set:\n",
    "output_csv_name = \"validation_set.csv\"\n",
    "generate_subset_csv(val_ds, output_csv_name, output_folder)\n",
    "\n",
    "# Train set:\n",
    "output_csv_name = \"train_set.csv\"\n",
    "generate_subset_csv(train_ds, output_csv_name, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/test_set.csv'))\n",
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/validation_set.csv'))\n",
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/train_set.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
